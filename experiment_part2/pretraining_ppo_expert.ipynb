{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from stable_baselines3 import PPO\n",
    "import stable_baselines3.ppo.policies as ppo\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import random \n",
    "from gym.envs.box2d.lunar_lander import heuristic as lunar_heuristic\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from parameter_initialization_deep_rl.common.helpers import create_folder\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from torch.nn.modules.activation import Sigmoid, Tanh, ReLU, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_naive(obs: np.ndarray):\n",
    "    obs = obs[0]\n",
    "    action = 0\n",
    "    if obs[1] < 0.3:\n",
    "        if obs[3] < -0.05:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 0\n",
    "    else:\n",
    "        a = random.random()\n",
    "        if a < 0.4 and obs[3] < 0:\n",
    "            action = 2\n",
    "        else:\n",
    "            if obs[4] < -0.1:\n",
    "                action = 1\n",
    "            elif obs[4] > 0.1:\n",
    "                action = 3\n",
    "    action = np.full((1),action)\n",
    "    return action\n",
    "\n",
    "def heuristic_expert(o: np.ndarray):\n",
    "    # o is single observation and of shape (1,8)\n",
    "    # action a has to be of shape (1)\n",
    "    a = np.full((1),lunar_heuristic(gym.make(\"LunarLander-v2\"), o[0]))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [95,  39,   1,  38,  83,  38,  36,  50,  14, 100,  67,  34,  68, 47,  70,  57,  64,  98,  93,  25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/ppo/bc_expert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(initial_value):\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    :param initial_value: (float or str)\n",
    "    :return: (function)\n",
    "    \"\"\"\n",
    "    if isinstance(initial_value, str):\n",
    "        initial_value = float(initial_value)\n",
    "\n",
    "    def func(progress):\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0\n",
    "        :param progress: (float)\n",
    "        :return: (float)\n",
    "        \"\"\"\n",
    "        return progress * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "hyperparameter = dict(\n",
    "    n_steps = 16,\n",
    "    gae_lambda = 0.98,\n",
    "    gamma = 0.99,\n",
    "    n_epochs = 4,\n",
    "    ent_coef = 0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpausche/miniconda3/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:146: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 40`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 40\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=5 and n_envs=8)\n",
      "  warnings.warn(\n",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 145      |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "475batch [00:01, 333.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000659 |\n",
      "|    entropy        | 0.659     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 184       |\n",
      "|    loss           | 0.68      |\n",
      "|    neglogp        | 0.681     |\n",
      "|    prob_true_act  | 0.65      |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "979batch [00:03, 234.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000434 |\n",
      "|    entropy        | 0.434     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 216       |\n",
      "|    loss           | 0.381     |\n",
      "|    neglogp        | 0.382     |\n",
      "|    prob_true_act  | 0.777     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1479batch [00:05, 229.05batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000496 |\n",
      "|    entropy        | 0.496     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 240       |\n",
      "|    loss           | 0.711     |\n",
      "|    neglogp        | 0.711     |\n",
      "|    prob_true_act  | 0.638     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1548batch [00:05, 227.84batch/s]\n",
      "1557batch [00:05, 262.38batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: 203.70084575\n",
      "Eval num_timesteps=25000, episode_reward=-98.93 +/- 67.77\n",
      "Episode length: 411.05 +/- 77.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-38.42 +/- 113.07\n",
      "Episode length: 788.30 +/- 307.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=26.15 +/- 138.79\n",
      "Episode length: 444.45 +/- 358.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=71.57 +/- 116.56\n",
      "Episode length: 271.40 +/- 239.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=-13.89 +/- 106.45\n",
      "Episode length: 746.55 +/- 389.03\n",
      "Eval num_timesteps=150000, episode_reward=126.59 +/- 103.07\n",
      "Episode length: 273.80 +/- 171.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=137.94 +/- 119.79\n",
      "Episode length: 328.30 +/- 205.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=39.05 +/- 110.31\n",
      "Episode length: 697.10 +/- 328.05\n",
      "Eval num_timesteps=225000, episode_reward=78.00 +/- 158.15\n",
      "Episode length: 474.40 +/- 299.09\n",
      "Eval num_timesteps=250000, episode_reward=28.75 +/- 109.93\n",
      "Episode length: 390.85 +/- 351.30\n",
      "Eval num_timesteps=275000, episode_reward=49.57 +/- 152.27\n",
      "Episode length: 192.55 +/- 113.13\n",
      "Eval num_timesteps=300000, episode_reward=-45.54 +/- 146.84\n",
      "Episode length: 358.05 +/- 309.80\n",
      "Eval num_timesteps=325000, episode_reward=76.73 +/- 137.39\n",
      "Episode length: 428.60 +/- 337.12\n",
      "Eval num_timesteps=350000, episode_reward=92.12 +/- 148.47\n",
      "Episode length: 355.80 +/- 263.90\n",
      "Eval num_timesteps=375000, episode_reward=-59.17 +/- 79.50\n",
      "Episode length: 888.85 +/- 267.87\n",
      "Eval num_timesteps=400000, episode_reward=66.47 +/- 126.99\n",
      "Episode length: 396.95 +/- 128.54\n",
      "Eval num_timesteps=425000, episode_reward=-11.58 +/- 50.82\n",
      "Episode length: 996.65 +/- 14.60\n",
      "Eval num_timesteps=450000, episode_reward=160.94 +/- 112.18\n",
      "Episode length: 339.95 +/- 232.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=475000, episode_reward=140.15 +/- 130.95\n",
      "Episode length: 529.85 +/- 279.78\n",
      "Eval num_timesteps=500000, episode_reward=164.45 +/- 111.89\n",
      "Episode length: 280.75 +/- 177.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=525000, episode_reward=173.01 +/- 100.31\n",
      "Episode length: 321.35 +/- 230.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=550000, episode_reward=222.00 +/- 63.93\n",
      "Episode length: 295.45 +/- 221.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=575000, episode_reward=154.14 +/- 87.03\n",
      "Episode length: 414.35 +/- 219.27\n",
      "Eval num_timesteps=600000, episode_reward=82.85 +/- 99.01\n",
      "Episode length: 637.25 +/- 353.91\n",
      "Eval num_timesteps=625000, episode_reward=31.80 +/- 98.35\n",
      "Episode length: 813.55 +/- 299.16\n",
      "Eval num_timesteps=650000, episode_reward=80.28 +/- 116.66\n",
      "Episode length: 716.15 +/- 309.69\n",
      "Eval num_timesteps=675000, episode_reward=153.02 +/- 100.66\n",
      "Episode length: 557.65 +/- 252.48\n",
      "Eval num_timesteps=700000, episode_reward=101.79 +/- 94.93\n",
      "Episode length: 615.05 +/- 294.55\n",
      "Eval num_timesteps=725000, episode_reward=106.34 +/- 103.76\n",
      "Episode length: 698.00 +/- 260.43\n",
      "Eval num_timesteps=750000, episode_reward=160.93 +/- 90.08\n",
      "Episode length: 387.75 +/- 153.61\n",
      "Eval num_timesteps=775000, episode_reward=195.24 +/- 79.21\n",
      "Episode length: 386.50 +/- 113.48\n",
      "Eval num_timesteps=800000, episode_reward=163.61 +/- 107.69\n",
      "Episode length: 320.75 +/- 184.42\n",
      "Eval num_timesteps=825000, episode_reward=161.51 +/- 110.38\n",
      "Episode length: 329.80 +/- 179.22\n",
      "Eval num_timesteps=850000, episode_reward=141.37 +/- 113.30\n",
      "Episode length: 307.10 +/- 175.75\n",
      "Eval num_timesteps=875000, episode_reward=123.84 +/- 102.02\n",
      "Episode length: 263.15 +/- 93.52\n",
      "Eval num_timesteps=900000, episode_reward=166.93 +/- 104.79\n",
      "Episode length: 306.00 +/- 228.75\n",
      "Eval num_timesteps=925000, episode_reward=141.12 +/- 105.37\n",
      "Episode length: 288.85 +/- 174.06\n",
      "Eval num_timesteps=950000, episode_reward=148.08 +/- 110.89\n",
      "Episode length: 279.85 +/- 183.32\n",
      "Eval num_timesteps=975000, episode_reward=159.09 +/- 118.31\n",
      "Episode length: 325.85 +/- 172.92\n",
      "Eval num_timesteps=1000000, episode_reward=142.20 +/- 114.82\n",
      "Episode length: 338.85 +/- 232.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 145      |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.38     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "486batch [00:01, 296.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | -0.0005  |\n",
      "|    entropy        | 0.5      |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 177      |\n",
      "|    loss           | 0.606    |\n",
      "|    neglogp        | 0.606    |\n",
      "|    prob_true_act  | 0.695    |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "979batch [00:03, 325.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000621 |\n",
      "|    entropy        | 0.621     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 203       |\n",
      "|    loss           | 0.514     |\n",
      "|    neglogp        | 0.515     |\n",
      "|    prob_true_act  | 0.673     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1471batch [00:04, 324.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000353 |\n",
      "|    entropy        | 0.353     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 224       |\n",
      "|    loss           | 0.169     |\n",
      "|    neglogp        | 0.169     |\n",
      "|    prob_true_act  | 0.863     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1570batch [00:05, 323.78batch/s]\n",
      "1581batch [00:05, 301.88batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: 162.9970681\n",
      "Eval num_timesteps=25000, episode_reward=44.05 +/- 177.52\n",
      "Episode length: 478.35 +/- 298.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-46.17 +/- 90.04\n",
      "Episode length: 279.30 +/- 111.42\n",
      "Eval num_timesteps=75000, episode_reward=45.82 +/- 180.06\n",
      "Episode length: 490.95 +/- 316.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=92.64 +/- 115.04\n",
      "Episode length: 174.65 +/- 76.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=57.09 +/- 103.24\n",
      "Episode length: 345.45 +/- 335.89\n",
      "Eval num_timesteps=150000, episode_reward=41.86 +/- 111.84\n",
      "Episode length: 186.95 +/- 187.72\n",
      "Eval num_timesteps=175000, episode_reward=117.59 +/- 89.23\n",
      "Episode length: 505.10 +/- 372.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=78.61 +/- 126.10\n",
      "Episode length: 384.10 +/- 380.07\n",
      "Eval num_timesteps=225000, episode_reward=85.82 +/- 156.37\n",
      "Episode length: 303.10 +/- 249.57\n",
      "Eval num_timesteps=250000, episode_reward=132.55 +/- 111.38\n",
      "Episode length: 236.45 +/- 185.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=39.33 +/- 130.60\n",
      "Episode length: 572.95 +/- 330.76\n",
      "Eval num_timesteps=300000, episode_reward=164.43 +/- 112.83\n",
      "Episode length: 363.00 +/- 169.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=206.79 +/- 52.85\n",
      "Episode length: 455.05 +/- 84.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=181.15 +/- 103.30\n",
      "Episode length: 395.70 +/- 205.46\n",
      "Eval num_timesteps=375000, episode_reward=145.49 +/- 122.64\n",
      "Episode length: 206.25 +/- 89.37\n",
      "Eval num_timesteps=400000, episode_reward=148.81 +/- 96.07\n",
      "Episode length: 341.35 +/- 161.93\n",
      "Eval num_timesteps=425000, episode_reward=195.60 +/- 76.18\n",
      "Episode length: 369.45 +/- 117.55\n",
      "Eval num_timesteps=450000, episode_reward=211.19 +/- 80.21\n",
      "Episode length: 248.45 +/- 80.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=475000, episode_reward=168.35 +/- 96.52\n",
      "Episode length: 283.45 +/- 101.70\n",
      "Eval num_timesteps=500000, episode_reward=198.12 +/- 100.11\n",
      "Episode length: 233.65 +/- 70.89\n",
      "Eval num_timesteps=525000, episode_reward=173.81 +/- 107.64\n",
      "Episode length: 249.95 +/- 184.01\n",
      "Eval num_timesteps=550000, episode_reward=193.40 +/- 98.63\n",
      "Episode length: 219.55 +/- 77.67\n",
      "Eval num_timesteps=575000, episode_reward=146.31 +/- 104.28\n",
      "Episode length: 277.65 +/- 245.67\n",
      "Eval num_timesteps=600000, episode_reward=165.66 +/- 111.95\n",
      "Episode length: 216.50 +/- 188.07\n",
      "Eval num_timesteps=625000, episode_reward=195.94 +/- 131.69\n",
      "Episode length: 211.05 +/- 51.96\n",
      "Eval num_timesteps=650000, episode_reward=239.54 +/- 59.75\n",
      "Episode length: 305.80 +/- 162.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=675000, episode_reward=199.24 +/- 96.98\n",
      "Episode length: 236.10 +/- 182.24\n",
      "Eval num_timesteps=700000, episode_reward=238.02 +/- 72.61\n",
      "Episode length: 277.50 +/- 167.16\n",
      "Eval num_timesteps=725000, episode_reward=232.14 +/- 69.55\n",
      "Episode length: 252.75 +/- 174.84\n",
      "Eval num_timesteps=750000, episode_reward=195.95 +/- 108.35\n",
      "Episode length: 345.20 +/- 292.20\n",
      "Eval num_timesteps=775000, episode_reward=200.24 +/- 93.87\n",
      "Episode length: 321.75 +/- 229.61\n",
      "Eval num_timesteps=800000, episode_reward=219.35 +/- 42.37\n",
      "Episode length: 415.25 +/- 108.63\n"
     ]
    }
   ],
   "source": [
    "reward_sum = 0\n",
    "\n",
    "for seed in seeds:\n",
    "    # create environments and set seed\n",
    "    env_train = gym.make(\"LunarLander-v2\")\n",
    "    env_train.seed(seed)\n",
    "    env_test = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "    env_test.seed(seed)\n",
    "    \n",
    "    # Collect rollouts using the expert\n",
    "    rollouts = rollout.rollout(\n",
    "        heuristic_expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(env_train)]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=200),\n",
    "    )\n",
    "    # Flatten the trajectories to obtain individual transitions\n",
    "    transitions = rollout.flatten_trajectories(rollouts)\n",
    "    # Set up student\n",
    "    student = PPO(\n",
    "        policy=ppo.MlpPolicy,\n",
    "        env=make_vec_env(env_id=\"LunarLander-v2\", n_envs=8, seed=seed),\n",
    "        gamma = 0.995,\n",
    "        n_steps = 5,\n",
    "        learning_rate = linear_schedule(0.00083),\n",
    "        ent_coef = 0.00001,\n",
    "        seed=seed,\n",
    "        verbose=False,\n",
    "    )\n",
    "    # Set up behavior cloning agent\n",
    "    bc_trainer = bc.BC(\n",
    "        observation_space=env_train.observation_space,\n",
    "        action_space=env_train.action_space,\n",
    "        demonstrations=transitions,\n",
    "        policy=student.policy,\n",
    "    )\n",
    "    # Pretrain the policy with behavior cloning\n",
    "    bc_trainer.train(n_epochs=1)\n",
    "    # Test performance before training\n",
    "    reward_before_training, _ = evaluate_policy(student.policy, env_test, 20)\n",
    "    reward_sum += reward_before_training\n",
    "    print(f\"0 Timesteps: {reward_before_training}\")\n",
    "    # Create log folder\n",
    "    trial_log_dir = create_folder(f\"{log_dir}/{seed}\")\n",
    "    # Create eval callback\n",
    "    eval_callback = EvalCallback(\n",
    "        env_test,\n",
    "        log_path=trial_log_dir,\n",
    "        n_eval_episodes=20,\n",
    "        eval_freq=3125,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    # Train the pretrained policy using the regular learning algorithm\n",
    "    student.learn(\n",
    "        total_timesteps=1e6,\n",
    "        callback=eval_callback\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameter_initialization_deep_rl.common.evaluate import (\n",
    "    create_numpy_arr_from_logs,\n",
    "    create_sample,\n",
    "    mean_confidence_intervals\n",
    ")\n",
    "from parameter_initialization_deep_rl.common.helpers import plot_performance\n",
    "\n",
    "log_dirs = [f\"{log_dir}/{seed}/evaluations.npz\" for seed in seeds]\n",
    "avg_returns, n = create_numpy_arr_from_logs(log_dirs)\n",
    "\n",
    "sample = create_sample(avg_returns)\n",
    "\n",
    "# Save the performance score for the individual seeds\n",
    "np.savez(f\"{log_dir}/avg_perf_across_seeds\",\n",
    "            sample=sample)\n",
    "\n",
    "performance_score = np.average(sample)\n",
    "\n",
    "# Save the performance score, i.e., the averaged return across all seeds and evaluation periods, i.e., just one single number\n",
    "np.savez(f\"{log_dir}/perf_score\",\n",
    "            perf_score=performance_score)\n",
    "\n",
    "AVG, H = mean_confidence_intervals(avg_returns, n)\n",
    "\n",
    "reward_before_training = reward_sum / len(seeds)\n",
    "AVG = np.append(reward_before_training, AVG)\n",
    "\n",
    "# Save the average return and confidence intervals for all the individual evaluation trials averaged across all seeds\n",
    "np.savez(f\"{log_dir}/avg_perf_eval_trials\", avg=AVG,\n",
    "            h=H)\n",
    "\n",
    "plot_performance(\n",
    "    title=\"Performance\",\n",
    "    graph_label=\"Random\",\n",
    "    x = np.arange(0,62*16384,16364),\n",
    "    y=AVG,\n",
    "    x_label=\"timesteps\",\n",
    "    y_label=\"Return\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AVG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dbef0c893a59910565eb9c43a000ae2fe143ab4038181e38f0de48b2404771b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
