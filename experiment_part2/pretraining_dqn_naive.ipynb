{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from stable_baselines3 import PPO, DQN\n",
    "import stable_baselines3.ppo.policies as ppo\n",
    "import stable_baselines3.dqn.policies as dqn\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gym\n",
    "import numpy as np\n",
    "import random \n",
    "from gym.envs.box2d.lunar_lander import heuristic as lunar_heuristic\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from parameter_initialization_deep_rl.common.helpers import create_folder\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from torch.nn.modules.activation import Sigmoid, Tanh, ReLU, LeakyReLU\n",
    "from torch import nn\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_naive(obs: np.ndarray):\n",
    "    obs = obs[0]\n",
    "    action = 0\n",
    "    if obs[1] < 0.3:\n",
    "        if obs[3] < -0.05:\n",
    "            action = 2\n",
    "        else:\n",
    "            action = 0\n",
    "    else:\n",
    "        a = random.random()\n",
    "        if a < 0.4 and obs[3] < 0:\n",
    "            action = 2\n",
    "        else:\n",
    "            if obs[4] < -0.1:\n",
    "                action = 1\n",
    "            elif obs[4] > 0.1:\n",
    "                action = 3\n",
    "    action = np.full((1),action)\n",
    "    return action\n",
    "\n",
    "def heuristic_expert(o: np.ndarray):\n",
    "    # o is single observation and of shape (1,8)\n",
    "    # action a has to be of shape (1)\n",
    "    a = np.full((1),lunar_heuristic(gym.make(\"LunarLander-v2\"), o[0]))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [95,  39,   1,  38,  83,  38,  36,  50,  14, 100,  67,  34,  68, 47,  70,  57,  64,  98,  93,  25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/dqn/bc_naive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "hyperparameter = dict(\n",
    "    n_steps = 16,\n",
    "    gae_lambda = 0.98,\n",
    "    gamma = 0.99,\n",
    "    n_epochs = 4,\n",
    "    ent_coef = 0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = dict(\n",
    "    learning_rate = 6.3e-4,\n",
    "    batch_size = 128,\n",
    "    buffer_size = 50000,\n",
    "    learning_starts = 0,\n",
    "    gamma = 0.99,\n",
    "    target_update_interval = 250,\n",
    "    train_freq = 4,\n",
    "    gradient_steps = -1,\n",
    "    exploration_fraction = 0.12,\n",
    "    exploration_final_eps = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.39     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "494batch [00:01, 396.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000849 |\n",
      "|    entropy        | 0.849     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 55.4      |\n",
      "|    loss           | 1.16      |\n",
      "|    neglogp        | 1.16      |\n",
      "|    prob_true_act  | 0.454     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934batch [00:02, 391.93batch/s]\n",
      "974batch [00:02, 387.61batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000865 |\n",
      "|    entropy        | 0.865     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 69.6      |\n",
      "|    loss           | 0.878     |\n",
      "|    neglogp        | 0.879     |\n",
      "|    prob_true_act  | 0.494     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1494batch [00:03, 395.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1500     |\n",
      "|    ent_loss       | -0.00073 |\n",
      "|    entropy        | 0.73     |\n",
      "|    epoch          | 1        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 84.4     |\n",
      "|    loss           | 0.77     |\n",
      "|    neglogp        | 0.77     |\n",
      "|    prob_true_act  | 0.569    |\n",
      "|    samples_so_far | 48032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1894batch [00:04, 393.34batch/s]\n",
      "1898batch [00:04, 388.80batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -118.13965919999998\n",
      "Eval num_timesteps=3750, episode_reward=-199.18 +/- 135.36\n",
      "Episode length: 268.50 +/- 130.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-176.71 +/- 110.25\n",
      "Episode length: 260.20 +/- 150.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-154.02 +/- 81.63\n",
      "Episode length: 555.55 +/- 342.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-61.68 +/- 36.38\n",
      "Episode length: 929.90 +/- 210.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18750, episode_reward=-58.81 +/- 20.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22500, episode_reward=-53.25 +/- 19.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26250, episode_reward=-34.78 +/- 23.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-45.57 +/- 32.07\n",
      "Episode length: 958.90 +/- 179.15\n",
      "Eval num_timesteps=33750, episode_reward=-59.27 +/- 62.06\n",
      "Episode length: 994.45 +/- 24.19\n",
      "Eval num_timesteps=37500, episode_reward=-82.55 +/- 49.47\n",
      "Episode length: 967.30 +/- 142.54\n",
      "Eval num_timesteps=41250, episode_reward=-82.84 +/- 93.51\n",
      "Episode length: 792.70 +/- 271.94\n",
      "Eval num_timesteps=45000, episode_reward=-50.69 +/- 20.86\n",
      "Episode length: 921.45 +/- 235.69\n",
      "Eval num_timesteps=48750, episode_reward=-39.64 +/- 77.09\n",
      "Episode length: 794.45 +/- 287.44\n",
      "Eval num_timesteps=52500, episode_reward=-33.11 +/- 64.14\n",
      "Episode length: 715.90 +/- 323.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56250, episode_reward=-62.24 +/- 73.65\n",
      "Episode length: 870.65 +/- 259.65\n",
      "Eval num_timesteps=60000, episode_reward=-72.16 +/- 120.40\n",
      "Episode length: 877.20 +/- 231.40\n",
      "Eval num_timesteps=63750, episode_reward=114.71 +/- 94.49\n",
      "Episode length: 838.20 +/- 288.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67500, episode_reward=35.46 +/- 212.36\n",
      "Episode length: 548.65 +/- 306.99\n",
      "Eval num_timesteps=71250, episode_reward=167.08 +/- 90.22\n",
      "Episode length: 637.20 +/- 260.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=100.28 +/- 91.61\n",
      "Episode length: 814.75 +/- 310.56\n",
      "Eval num_timesteps=78750, episode_reward=-7.57 +/- 18.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=82500, episode_reward=16.61 +/- 84.69\n",
      "Episode length: 867.65 +/- 241.83\n",
      "Eval num_timesteps=86250, episode_reward=27.84 +/- 91.01\n",
      "Episode length: 668.85 +/- 390.35\n",
      "Eval num_timesteps=90000, episode_reward=23.73 +/- 66.73\n",
      "Episode length: 663.45 +/- 418.80\n",
      "Eval num_timesteps=93750, episode_reward=150.31 +/- 96.31\n",
      "Episode length: 508.95 +/- 295.69\n",
      "Eval num_timesteps=97500, episode_reward=164.00 +/- 112.92\n",
      "Episode length: 332.65 +/- 178.88\n",
      "Eval num_timesteps=101250, episode_reward=225.19 +/- 52.84\n",
      "Episode length: 474.05 +/- 154.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=253.16 +/- 27.67\n",
      "Episode length: 349.00 +/- 77.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108750, episode_reward=245.92 +/- 27.34\n",
      "Episode length: 362.40 +/- 73.12\n",
      "Eval num_timesteps=112500, episode_reward=204.97 +/- 77.39\n",
      "Episode length: 468.65 +/- 243.73\n",
      "Eval num_timesteps=116250, episode_reward=190.42 +/- 100.79\n",
      "Episode length: 525.25 +/- 265.40\n",
      "Eval num_timesteps=120000, episode_reward=200.77 +/- 107.27\n",
      "Episode length: 433.55 +/- 229.35\n",
      "Eval num_timesteps=123750, episode_reward=137.07 +/- 155.98\n",
      "Episode length: 539.35 +/- 299.40\n",
      "Eval num_timesteps=127500, episode_reward=111.69 +/- 112.62\n",
      "Episode length: 732.95 +/- 313.34\n",
      "Eval num_timesteps=131250, episode_reward=140.94 +/- 117.49\n",
      "Episode length: 611.10 +/- 355.86\n",
      "Eval num_timesteps=135000, episode_reward=175.25 +/- 96.95\n",
      "Episode length: 492.65 +/- 278.08\n",
      "Eval num_timesteps=138750, episode_reward=91.06 +/- 135.17\n",
      "Episode length: 691.40 +/- 347.89\n",
      "Eval num_timesteps=142500, episode_reward=132.09 +/- 130.35\n",
      "Episode length: 400.65 +/- 312.82\n",
      "Eval num_timesteps=146250, episode_reward=200.69 +/- 124.98\n",
      "Episode length: 358.35 +/- 214.24\n",
      "Eval num_timesteps=150000, episode_reward=242.22 +/- 69.49\n",
      "Episode length: 391.40 +/- 237.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469batch [00:01, 376.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000903 |\n",
      "|    entropy        | 0.903     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 53.4      |\n",
      "|    loss           | 0.883     |\n",
      "|    neglogp        | 0.884     |\n",
      "|    prob_true_act  | 0.483     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "897batch [00:02, 380.56batch/s]\n",
      "975batch [00:02, 375.45batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000856 |\n",
      "|    entropy        | 0.856     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 65.9      |\n",
      "|    loss           | 0.632     |\n",
      "|    neglogp        | 0.633     |\n",
      "|    prob_true_act  | 0.556     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1475batch [00:03, 379.79batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000846 |\n",
      "|    entropy        | 0.846     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 80.3      |\n",
      "|    loss           | 0.672     |\n",
      "|    neglogp        | 0.673     |\n",
      "|    prob_true_act  | 0.561     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1819batch [00:04, 379.77batch/s]\n",
      "1838batch [00:04, 372.49batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -153.27564109999997\n",
      "Eval num_timesteps=3750, episode_reward=-126.34 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-114.53 +/- 169.10\n",
      "Episode length: 683.70 +/- 283.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-127.64 +/- 78.26\n",
      "Episode length: 354.55 +/- 174.77\n",
      "Eval num_timesteps=15000, episode_reward=-163.98 +/- 52.18\n",
      "Episode length: 288.15 +/- 71.55\n",
      "Eval num_timesteps=18750, episode_reward=-181.32 +/- 33.66\n",
      "Episode length: 435.15 +/- 125.15\n",
      "Eval num_timesteps=22500, episode_reward=-131.85 +/- 63.68\n",
      "Episode length: 777.25 +/- 274.58\n",
      "Eval num_timesteps=26250, episode_reward=-34.28 +/- 23.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-57.44 +/- 87.72\n",
      "Episode length: 670.75 +/- 283.50\n",
      "Eval num_timesteps=33750, episode_reward=-27.86 +/- 33.23\n",
      "Episode length: 979.85 +/- 87.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37500, episode_reward=-24.88 +/- 32.11\n",
      "Episode length: 961.75 +/- 166.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41250, episode_reward=3.12 +/- 55.21\n",
      "Episode length: 977.30 +/- 98.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-12.79 +/- 22.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=48750, episode_reward=-5.76 +/- 17.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=-3.11 +/- 23.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=56250, episode_reward=-5.02 +/- 26.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-7.78 +/- 22.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=63750, episode_reward=-27.48 +/- 30.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=-15.92 +/- 41.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=71250, episode_reward=-30.53 +/- 81.81\n",
      "Episode length: 762.75 +/- 310.68\n",
      "Eval num_timesteps=75000, episode_reward=24.86 +/- 124.46\n",
      "Episode length: 865.80 +/- 224.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78750, episode_reward=-19.59 +/- 77.94\n",
      "Episode length: 827.40 +/- 240.71\n",
      "Eval num_timesteps=82500, episode_reward=62.71 +/- 148.48\n",
      "Episode length: 757.15 +/- 234.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86250, episode_reward=203.37 +/- 104.50\n",
      "Episode length: 508.85 +/- 105.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=194.59 +/- 94.14\n",
      "Episode length: 568.40 +/- 129.62\n",
      "Eval num_timesteps=93750, episode_reward=206.29 +/- 52.88\n",
      "Episode length: 501.25 +/- 103.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=97500, episode_reward=244.47 +/- 26.72\n",
      "Episode length: 369.45 +/- 68.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101250, episode_reward=239.21 +/- 18.18\n",
      "Episode length: 419.40 +/- 76.87\n",
      "Eval num_timesteps=105000, episode_reward=43.20 +/- 128.12\n",
      "Episode length: 842.60 +/- 243.11\n",
      "Eval num_timesteps=108750, episode_reward=153.20 +/- 84.40\n",
      "Episode length: 724.80 +/- 197.49\n",
      "Eval num_timesteps=112500, episode_reward=124.97 +/- 156.99\n",
      "Episode length: 450.50 +/- 161.42\n",
      "Eval num_timesteps=116250, episode_reward=158.00 +/- 91.57\n",
      "Episode length: 420.10 +/- 275.79\n",
      "Eval num_timesteps=120000, episode_reward=70.67 +/- 163.59\n",
      "Episode length: 481.00 +/- 291.13\n",
      "Eval num_timesteps=123750, episode_reward=127.47 +/- 134.89\n",
      "Episode length: 540.10 +/- 290.84\n",
      "Eval num_timesteps=127500, episode_reward=80.96 +/- 179.99\n",
      "Episode length: 340.80 +/- 199.42\n",
      "Eval num_timesteps=131250, episode_reward=106.75 +/- 156.42\n",
      "Episode length: 318.75 +/- 90.08\n",
      "Eval num_timesteps=135000, episode_reward=150.25 +/- 128.69\n",
      "Episode length: 466.55 +/- 259.10\n",
      "Eval num_timesteps=138750, episode_reward=173.06 +/- 108.42\n",
      "Episode length: 413.50 +/- 133.13\n",
      "Eval num_timesteps=142500, episode_reward=130.19 +/- 124.42\n",
      "Episode length: 353.30 +/- 218.72\n",
      "Eval num_timesteps=146250, episode_reward=15.67 +/- 245.95\n",
      "Episode length: 318.90 +/- 139.47\n",
      "Eval num_timesteps=150000, episode_reward=96.76 +/- 129.00\n",
      "Episode length: 271.15 +/- 111.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.39     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "471batch [00:01, 377.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000873 |\n",
      "|    entropy        | 0.873     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 54.3      |\n",
      "|    loss           | 1.11      |\n",
      "|    neglogp        | 1.11      |\n",
      "|    prob_true_act  | 0.446     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "872batch [00:02, 339.80batch/s]\n",
      "986batch [00:02, 363.40batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000847 |\n",
      "|    entropy        | 0.847     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 67.8      |\n",
      "|    loss           | 0.833     |\n",
      "|    neglogp        | 0.834     |\n",
      "|    prob_true_act  | 0.541     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1489batch [00:04, 380.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000912 |\n",
      "|    entropy        | 0.912     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 81.5      |\n",
      "|    loss           | 0.991     |\n",
      "|    neglogp        | 0.991     |\n",
      "|    prob_true_act  | 0.458     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1794batch [00:04, 365.72batch/s]\n",
      "1816batch [00:05, 359.80batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -442.88972805000003\n",
      "Eval num_timesteps=3750, episode_reward=-139.11 +/- 27.72\n",
      "Episode length: 979.55 +/- 89.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-131.15 +/- 80.79\n",
      "Episode length: 412.70 +/- 101.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-162.83 +/- 63.71\n",
      "Episode length: 301.40 +/- 105.31\n",
      "Eval num_timesteps=15000, episode_reward=-224.83 +/- 116.57\n",
      "Episode length: 598.50 +/- 287.80\n",
      "Eval num_timesteps=18750, episode_reward=-82.74 +/- 33.46\n",
      "Episode length: 772.55 +/- 348.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22500, episode_reward=-58.80 +/- 25.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26250, episode_reward=-85.14 +/- 32.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-112.42 +/- 41.37\n",
      "Episode length: 972.05 +/- 121.83\n",
      "Eval num_timesteps=33750, episode_reward=-71.64 +/- 18.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=37500, episode_reward=-42.86 +/- 19.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41250, episode_reward=1.43 +/- 17.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-34.85 +/- 18.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=48750, episode_reward=-60.30 +/- 18.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=-41.86 +/- 21.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=56250, episode_reward=-56.53 +/- 21.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-30.98 +/- 29.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=63750, episode_reward=-87.34 +/- 52.09\n",
      "Episode length: 920.40 +/- 127.86\n",
      "Eval num_timesteps=67500, episode_reward=-98.11 +/- 153.82\n",
      "Episode length: 986.35 +/- 59.50\n",
      "Eval num_timesteps=71250, episode_reward=-42.44 +/- 30.66\n",
      "Episode length: 498.55 +/- 454.07\n",
      "Eval num_timesteps=75000, episode_reward=-11.36 +/- 26.91\n",
      "Episode length: 911.50 +/- 265.50\n",
      "Eval num_timesteps=78750, episode_reward=-50.48 +/- 25.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=82500, episode_reward=-6.99 +/- 19.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=86250, episode_reward=-13.21 +/- 21.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-12.67 +/- 16.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=93750, episode_reward=-6.17 +/- 23.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=97500, episode_reward=5.76 +/- 21.56\n",
      "Episode length: 827.30 +/- 345.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101250, episode_reward=23.56 +/- 49.47\n",
      "Episode length: 962.25 +/- 164.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=67.71 +/- 104.57\n",
      "Episode length: 631.75 +/- 370.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108750, episode_reward=78.59 +/- 105.67\n",
      "Episode length: 478.75 +/- 392.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=112500, episode_reward=16.58 +/- 19.99\n",
      "Episode length: 385.00 +/- 359.81\n",
      "Eval num_timesteps=116250, episode_reward=87.16 +/- 78.96\n",
      "Episode length: 446.05 +/- 370.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=164.28 +/- 99.42\n",
      "Episode length: 448.30 +/- 281.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123750, episode_reward=125.37 +/- 140.82\n",
      "Episode length: 282.60 +/- 207.34\n",
      "Eval num_timesteps=127500, episode_reward=205.06 +/- 76.73\n",
      "Episode length: 562.40 +/- 296.81\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131250, episode_reward=146.61 +/- 122.44\n",
      "Episode length: 634.45 +/- 331.43\n",
      "Eval num_timesteps=135000, episode_reward=87.17 +/- 117.14\n",
      "Episode length: 809.35 +/- 217.01\n",
      "Eval num_timesteps=138750, episode_reward=217.55 +/- 88.05\n",
      "Episode length: 365.25 +/- 179.71\n",
      "New best mean reward!\n",
      "Eval num_timesteps=142500, episode_reward=256.50 +/- 45.38\n",
      "Episode length: 256.65 +/- 42.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=146250, episode_reward=212.51 +/- 94.11\n",
      "Episode length: 292.35 +/- 98.33\n",
      "Eval num_timesteps=150000, episode_reward=193.70 +/- 116.55\n",
      "Episode length: 225.05 +/- 93.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.39     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "489batch [00:01, 374.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000901 |\n",
      "|    entropy        | 0.901     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 54.8      |\n",
      "|    loss           | 0.857     |\n",
      "|    neglogp        | 0.858     |\n",
      "|    prob_true_act  | 0.479     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900batch [00:02, 366.83batch/s]\n",
      "975batch [00:02, 366.52batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000823 |\n",
      "|    entropy        | 0.823     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 65.6      |\n",
      "|    loss           | 0.861     |\n",
      "|    neglogp        | 0.862     |\n",
      "|    prob_true_act  | 0.526     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1499batch [00:04, 364.16batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000842 |\n",
      "|    entropy        | 0.842     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 77.8      |\n",
      "|    loss           | 0.669     |\n",
      "|    neglogp        | 0.67      |\n",
      "|    prob_true_act  | 0.562     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1801batch [00:04, 372.44batch/s]\n",
      "1838batch [00:05, 367.06batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -235.84904410000004\n",
      "Eval num_timesteps=3750, episode_reward=-113.08 +/- 34.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-180.02 +/- 32.24\n",
      "Episode length: 586.20 +/- 130.82\n",
      "Eval num_timesteps=11250, episode_reward=-137.13 +/- 43.02\n",
      "Episode length: 544.45 +/- 179.57\n",
      "Eval num_timesteps=15000, episode_reward=-29.45 +/- 22.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18750, episode_reward=-36.62 +/- 25.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=22500, episode_reward=-59.38 +/- 24.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=26250, episode_reward=-51.66 +/- 24.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-69.98 +/- 19.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=33750, episode_reward=-34.16 +/- 64.67\n",
      "Episode length: 972.85 +/- 118.34\n",
      "Eval num_timesteps=37500, episode_reward=-23.10 +/- 25.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41250, episode_reward=-12.81 +/- 15.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-12.83 +/- 18.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=48750, episode_reward=-14.97 +/- 24.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=-18.69 +/- 22.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=56250, episode_reward=-22.95 +/- 21.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-22.25 +/- 19.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=63750, episode_reward=-47.63 +/- 63.44\n",
      "Episode length: 922.70 +/- 189.36\n",
      "Eval num_timesteps=67500, episode_reward=-35.64 +/- 47.06\n",
      "Episode length: 996.10 +/- 17.00\n",
      "Eval num_timesteps=71250, episode_reward=-14.56 +/- 32.64\n",
      "Episode length: 912.70 +/- 261.91\n",
      "Eval num_timesteps=75000, episode_reward=-12.70 +/- 19.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=78750, episode_reward=-62.48 +/- 51.72\n",
      "Episode length: 951.10 +/- 193.91\n",
      "Eval num_timesteps=82500, episode_reward=21.39 +/- 154.04\n",
      "Episode length: 760.80 +/- 326.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86250, episode_reward=46.12 +/- 144.92\n",
      "Episode length: 762.30 +/- 325.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=140.01 +/- 125.95\n",
      "Episode length: 641.20 +/- 315.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93750, episode_reward=17.29 +/- 90.59\n",
      "Episode length: 904.55 +/- 218.89\n",
      "Eval num_timesteps=97500, episode_reward=-62.85 +/- 244.92\n",
      "Episode length: 451.50 +/- 320.91\n",
      "Eval num_timesteps=101250, episode_reward=-116.73 +/- 111.30\n",
      "Episode length: 278.15 +/- 253.67\n",
      "Eval num_timesteps=105000, episode_reward=-189.98 +/- 67.60\n",
      "Episode length: 331.30 +/- 233.46\n",
      "Eval num_timesteps=108750, episode_reward=-146.91 +/- 97.06\n",
      "Episode length: 375.70 +/- 306.72\n",
      "Eval num_timesteps=112500, episode_reward=-112.53 +/- 116.27\n",
      "Episode length: 615.30 +/- 429.93\n",
      "Eval num_timesteps=116250, episode_reward=-28.49 +/- 23.94\n",
      "Episode length: 955.00 +/- 196.15\n",
      "Eval num_timesteps=120000, episode_reward=6.62 +/- 66.14\n",
      "Episode length: 892.85 +/- 255.57\n",
      "Eval num_timesteps=123750, episode_reward=-7.37 +/- 312.41\n",
      "Episode length: 408.45 +/- 288.63\n",
      "Eval num_timesteps=127500, episode_reward=110.14 +/- 154.28\n",
      "Episode length: 247.10 +/- 196.32\n",
      "Eval num_timesteps=131250, episode_reward=-35.40 +/- 182.83\n",
      "Episode length: 379.65 +/- 293.93\n",
      "Eval num_timesteps=135000, episode_reward=-71.47 +/- 143.43\n",
      "Episode length: 387.25 +/- 256.13\n",
      "Eval num_timesteps=138750, episode_reward=-28.64 +/- 164.24\n",
      "Episode length: 393.45 +/- 184.20\n",
      "Eval num_timesteps=142500, episode_reward=30.95 +/- 157.03\n",
      "Episode length: 691.55 +/- 335.31\n",
      "Eval num_timesteps=146250, episode_reward=-7.77 +/- 161.22\n",
      "Episode length: 430.45 +/- 240.03\n",
      "Eval num_timesteps=150000, episode_reward=-63.54 +/- 155.62\n",
      "Episode length: 611.00 +/- 251.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "491batch [00:01, 367.05batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | -0.001   |\n",
      "|    entropy        | 1        |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 54.1     |\n",
      "|    loss           | 0.891    |\n",
      "|    neglogp        | 0.892    |\n",
      "|    prob_true_act  | 0.46     |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "936batch [00:02, 367.49batch/s]\n",
      "973batch [00:02, 361.54batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000942 |\n",
      "|    entropy        | 0.942     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 66.8      |\n",
      "|    loss           | 0.803     |\n",
      "|    neglogp        | 0.804     |\n",
      "|    prob_true_act  | 0.499     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1499batch [00:04, 375.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000842 |\n",
      "|    entropy        | 0.842     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 83.4      |\n",
      "|    loss           | 0.679     |\n",
      "|    neglogp        | 0.679     |\n",
      "|    prob_true_act  | 0.561     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1916batch [00:05, 373.93batch/s]\n",
      "1930batch [00:05, 361.38batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -276.1607797\n",
      "Eval num_timesteps=3750, episode_reward=-101.85 +/- 23.69\n",
      "Episode length: 967.65 +/- 141.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-145.27 +/- 67.00\n",
      "Episode length: 580.15 +/- 232.24\n",
      "Eval num_timesteps=11250, episode_reward=-138.03 +/- 52.64\n",
      "Episode length: 379.45 +/- 171.47\n",
      "Eval num_timesteps=15000, episode_reward=-124.91 +/- 71.03\n",
      "Episode length: 527.00 +/- 221.28\n",
      "Eval num_timesteps=18750, episode_reward=-54.66 +/- 37.16\n",
      "Episode length: 958.00 +/- 183.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22500, episode_reward=-14.10 +/- 18.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26250, episode_reward=-13.84 +/- 33.38\n",
      "Episode length: 958.90 +/- 179.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.90 +/- 19.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33750, episode_reward=-4.37 +/- 16.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=37500, episode_reward=-1.62 +/- 92.23\n",
      "Episode length: 987.50 +/- 41.47\n",
      "Eval num_timesteps=41250, episode_reward=15.96 +/- 53.69\n",
      "Episode length: 979.05 +/- 91.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=182.20 +/- 150.49\n",
      "Episode length: 364.15 +/- 144.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=48750, episode_reward=169.64 +/- 141.23\n",
      "Episode length: 384.95 +/- 169.86\n",
      "Eval num_timesteps=52500, episode_reward=176.32 +/- 95.72\n",
      "Episode length: 437.10 +/- 260.26\n",
      "Eval num_timesteps=56250, episode_reward=189.56 +/- 77.03\n",
      "Episode length: 590.15 +/- 192.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=182.68 +/- 126.04\n",
      "Episode length: 477.05 +/- 153.56\n",
      "Eval num_timesteps=63750, episode_reward=241.05 +/- 19.45\n",
      "Episode length: 397.90 +/- 103.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=67500, episode_reward=223.43 +/- 45.03\n",
      "Episode length: 492.55 +/- 165.41\n",
      "Eval num_timesteps=71250, episode_reward=-11.75 +/- 41.90\n",
      "Episode length: 985.30 +/- 64.08\n",
      "Eval num_timesteps=75000, episode_reward=-50.66 +/- 12.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=78750, episode_reward=-32.95 +/- 53.23\n",
      "Episode length: 978.55 +/- 71.36\n",
      "Eval num_timesteps=82500, episode_reward=141.37 +/- 89.83\n",
      "Episode length: 483.70 +/- 297.99\n",
      "Eval num_timesteps=86250, episode_reward=14.00 +/- 95.88\n",
      "Episode length: 295.55 +/- 348.07\n",
      "Eval num_timesteps=90000, episode_reward=37.12 +/- 84.19\n",
      "Episode length: 725.95 +/- 367.63\n",
      "Eval num_timesteps=93750, episode_reward=18.28 +/- 69.84\n",
      "Episode length: 645.35 +/- 435.79\n",
      "Eval num_timesteps=97500, episode_reward=212.37 +/- 60.67\n",
      "Episode length: 523.10 +/- 204.82\n",
      "Eval num_timesteps=101250, episode_reward=36.96 +/- 90.00\n",
      "Episode length: 920.60 +/- 203.62\n",
      "Eval num_timesteps=105000, episode_reward=119.42 +/- 112.05\n",
      "Episode length: 802.30 +/- 265.29\n",
      "Eval num_timesteps=108750, episode_reward=9.37 +/- 186.12\n",
      "Episode length: 856.55 +/- 265.33\n",
      "Eval num_timesteps=112500, episode_reward=209.57 +/- 58.76\n",
      "Episode length: 513.70 +/- 220.11\n",
      "Eval num_timesteps=116250, episode_reward=179.56 +/- 108.67\n",
      "Episode length: 420.10 +/- 232.10\n",
      "Eval num_timesteps=120000, episode_reward=178.99 +/- 107.89\n",
      "Episode length: 351.30 +/- 219.45\n",
      "Eval num_timesteps=123750, episode_reward=149.33 +/- 118.51\n",
      "Episode length: 596.95 +/- 289.67\n",
      "Eval num_timesteps=127500, episode_reward=114.31 +/- 127.32\n",
      "Episode length: 704.25 +/- 316.69\n",
      "Eval num_timesteps=131250, episode_reward=143.63 +/- 98.31\n",
      "Episode length: 689.90 +/- 315.34\n",
      "Eval num_timesteps=135000, episode_reward=215.50 +/- 97.14\n",
      "Episode length: 308.60 +/- 202.06\n",
      "Eval num_timesteps=138750, episode_reward=180.26 +/- 107.36\n",
      "Episode length: 332.95 +/- 146.33\n",
      "Eval num_timesteps=142500, episode_reward=183.46 +/- 96.02\n",
      "Episode length: 327.00 +/- 236.93\n",
      "Eval num_timesteps=146250, episode_reward=235.38 +/- 82.89\n",
      "Episode length: 296.10 +/- 89.44\n",
      "Eval num_timesteps=150000, episode_reward=184.74 +/- 105.04\n",
      "Episode length: 505.90 +/- 305.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.39     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.249    |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "496batch [00:01, 368.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000925 |\n",
      "|    entropy        | 0.925     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 54.1      |\n",
      "|    loss           | 0.759     |\n",
      "|    neglogp        | 0.76      |\n",
      "|    prob_true_act  | 0.517     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "872batch [00:02, 370.13batch/s]\n",
      "986batch [00:02, 368.54batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | -0.00088 |\n",
      "|    entropy        | 0.88     |\n",
      "|    epoch          | 1        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 66.2     |\n",
      "|    loss           | 0.728    |\n",
      "|    neglogp        | 0.729    |\n",
      "|    prob_true_act  | 0.541    |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1479batch [00:04, 371.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000804 |\n",
      "|    entropy        | 0.804     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 79.3      |\n",
      "|    loss           | 0.82      |\n",
      "|    neglogp        | 0.821     |\n",
      "|    prob_true_act  | 0.554     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1783batch [00:04, 374.95batch/s]\n",
      "1798batch [00:04, 364.85batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -184.1612621\n",
      "Eval num_timesteps=3750, episode_reward=-237.36 +/- 28.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-195.58 +/- 81.22\n",
      "Episode length: 687.40 +/- 219.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-160.61 +/- 44.93\n",
      "Episode length: 309.30 +/- 52.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-93.26 +/- 26.33\n",
      "Episode length: 970.80 +/- 127.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18750, episode_reward=-29.77 +/- 18.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22500, episode_reward=-29.08 +/- 20.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26250, episode_reward=-28.18 +/- 25.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-14.44 +/- 20.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33750, episode_reward=-17.28 +/- 18.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=37500, episode_reward=-71.49 +/- 34.02\n",
      "Episode length: 982.30 +/- 77.15\n",
      "Eval num_timesteps=41250, episode_reward=-110.95 +/- 57.16\n",
      "Episode length: 655.90 +/- 250.38\n",
      "Eval num_timesteps=45000, episode_reward=-87.90 +/- 52.43\n",
      "Episode length: 803.15 +/- 295.27\n",
      "Eval num_timesteps=48750, episode_reward=-94.89 +/- 47.70\n",
      "Episode length: 783.55 +/- 336.86\n",
      "Eval num_timesteps=52500, episode_reward=-34.81 +/- 33.05\n",
      "Episode length: 961.30 +/- 168.69\n",
      "Eval num_timesteps=56250, episode_reward=-23.39 +/- 22.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-25.08 +/- 22.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=63750, episode_reward=-26.15 +/- 21.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=-14.27 +/- 20.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71250, episode_reward=-22.39 +/- 24.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-59.51 +/- 81.90\n",
      "Episode length: 613.10 +/- 402.97\n",
      "Eval num_timesteps=78750, episode_reward=-54.36 +/- 124.46\n",
      "Episode length: 953.15 +/- 173.65\n",
      "Eval num_timesteps=82500, episode_reward=2.98 +/- 51.48\n",
      "Episode length: 980.30 +/- 85.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86250, episode_reward=-13.36 +/- 26.56\n",
      "Episode length: 955.00 +/- 196.15\n",
      "Eval num_timesteps=90000, episode_reward=8.57 +/- 50.79\n",
      "Episode length: 806.60 +/- 347.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=93750, episode_reward=40.97 +/- 104.00\n",
      "Episode length: 196.95 +/- 213.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=97500, episode_reward=-15.51 +/- 61.34\n",
      "Episode length: 183.10 +/- 272.41\n",
      "Eval num_timesteps=101250, episode_reward=53.41 +/- 125.89\n",
      "Episode length: 500.20 +/- 392.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=-67.55 +/- 64.72\n",
      "Episode length: 111.30 +/- 83.43\n",
      "Eval num_timesteps=108750, episode_reward=-31.27 +/- 125.86\n",
      "Episode length: 274.30 +/- 266.21\n",
      "Eval num_timesteps=112500, episode_reward=47.65 +/- 114.28\n",
      "Episode length: 272.70 +/- 209.21\n",
      "Eval num_timesteps=116250, episode_reward=31.24 +/- 151.30\n",
      "Episode length: 393.20 +/- 285.93\n",
      "Eval num_timesteps=120000, episode_reward=-16.01 +/- 111.72\n",
      "Episode length: 342.95 +/- 242.03\n",
      "Eval num_timesteps=123750, episode_reward=-28.80 +/- 55.53\n",
      "Episode length: 262.70 +/- 154.75\n",
      "Eval num_timesteps=127500, episode_reward=-45.74 +/- 25.21\n",
      "Episode length: 972.65 +/- 119.22\n",
      "Eval num_timesteps=131250, episode_reward=-8.04 +/- 55.93\n",
      "Episode length: 963.00 +/- 104.64\n",
      "Eval num_timesteps=135000, episode_reward=-16.93 +/- 73.81\n",
      "Episode length: 931.60 +/- 164.49\n",
      "Eval num_timesteps=138750, episode_reward=-45.80 +/- 23.20\n",
      "Episode length: 945.15 +/- 130.82\n",
      "Eval num_timesteps=142500, episode_reward=-32.00 +/- 20.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=146250, episode_reward=90.46 +/- 95.56\n",
      "Episode length: 784.35 +/- 203.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=199.14 +/- 35.98\n",
      "Episode length: 600.55 +/- 139.13\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.39     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467batch [00:01, 364.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000934 |\n",
      "|    entropy        | 0.934     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 54.5      |\n",
      "|    loss           | 0.995     |\n",
      "|    neglogp        | 0.996     |\n",
      "|    prob_true_act  | 0.464     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "912batch [00:02, 367.88batch/s]\n",
      "987batch [00:02, 365.29batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000856 |\n",
      "|    entropy        | 0.856     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 68.5      |\n",
      "|    loss           | 0.719     |\n",
      "|    neglogp        | 0.72      |\n",
      "|    prob_true_act  | 0.564     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1472batch [00:04, 368.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000755 |\n",
      "|    entropy        | 0.755     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 85.6      |\n",
      "|    loss           | 0.761     |\n",
      "|    neglogp        | 0.761     |\n",
      "|    prob_true_act  | 0.569     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1847batch [00:05, 371.33batch/s]\n",
      "1862batch [00:05, 360.25batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -263.1966156\n",
      "Eval num_timesteps=3750, episode_reward=-191.05 +/- 240.30\n",
      "Episode length: 876.50 +/- 214.85\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-40.31 +/- 148.23\n",
      "Episode length: 466.05 +/- 251.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-71.97 +/- 91.09\n",
      "Episode length: 284.40 +/- 131.35\n",
      "Eval num_timesteps=15000, episode_reward=-189.40 +/- 70.49\n",
      "Episode length: 234.65 +/- 73.00\n",
      "Eval num_timesteps=18750, episode_reward=-169.50 +/- 24.64\n",
      "Episode length: 370.45 +/- 78.59\n",
      "Eval num_timesteps=22500, episode_reward=-82.25 +/- 23.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=26250, episode_reward=-136.09 +/- 132.19\n",
      "Episode length: 876.55 +/- 293.91\n",
      "Eval num_timesteps=30000, episode_reward=-33.85 +/- 26.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33750, episode_reward=-6.90 +/- 18.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37500, episode_reward=-10.77 +/- 19.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=41250, episode_reward=6.77 +/- 81.53\n",
      "Episode length: 938.25 +/- 185.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-21.15 +/- 72.39\n",
      "Episode length: 923.60 +/- 229.35\n",
      "Eval num_timesteps=48750, episode_reward=47.96 +/- 109.19\n",
      "Episode length: 790.80 +/- 301.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52500, episode_reward=42.82 +/- 117.40\n",
      "Episode length: 227.70 +/- 85.31\n",
      "Eval num_timesteps=56250, episode_reward=-94.71 +/- 87.72\n",
      "Episode length: 187.50 +/- 29.49\n",
      "Eval num_timesteps=60000, episode_reward=-41.60 +/- 73.57\n",
      "Episode length: 177.65 +/- 43.55\n",
      "Eval num_timesteps=63750, episode_reward=-62.58 +/- 105.48\n",
      "Episode length: 248.75 +/- 136.92\n",
      "Eval num_timesteps=67500, episode_reward=-166.43 +/- 191.93\n",
      "Episode length: 197.10 +/- 86.38\n",
      "Eval num_timesteps=71250, episode_reward=-229.86 +/- 193.23\n",
      "Episode length: 181.05 +/- 191.12\n",
      "Eval num_timesteps=75000, episode_reward=-84.25 +/- 305.72\n",
      "Episode length: 212.35 +/- 180.99\n",
      "Eval num_timesteps=78750, episode_reward=108.81 +/- 168.15\n",
      "Episode length: 268.85 +/- 184.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=82500, episode_reward=2.32 +/- 139.51\n",
      "Episode length: 654.30 +/- 326.54\n",
      "Eval num_timesteps=86250, episode_reward=69.30 +/- 204.46\n",
      "Episode length: 442.10 +/- 195.13\n",
      "Eval num_timesteps=90000, episode_reward=-15.47 +/- 91.05\n",
      "Episode length: 334.25 +/- 342.09\n",
      "Eval num_timesteps=93750, episode_reward=-58.36 +/- 85.06\n",
      "Episode length: 962.50 +/- 112.55\n",
      "Eval num_timesteps=97500, episode_reward=-38.44 +/- 19.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=101250, episode_reward=-8.91 +/- 17.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=129.53 +/- 167.66\n",
      "Episode length: 514.30 +/- 125.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108750, episode_reward=-14.34 +/- 280.02\n",
      "Episode length: 779.80 +/- 237.48\n",
      "Eval num_timesteps=112500, episode_reward=36.15 +/- 124.07\n",
      "Episode length: 536.00 +/- 340.94\n",
      "Eval num_timesteps=116250, episode_reward=5.62 +/- 124.51\n",
      "Episode length: 432.25 +/- 338.58\n",
      "Eval num_timesteps=120000, episode_reward=-25.27 +/- 134.10\n",
      "Episode length: 356.70 +/- 278.10\n",
      "Eval num_timesteps=123750, episode_reward=175.52 +/- 181.56\n",
      "Episode length: 319.60 +/- 113.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=127500, episode_reward=214.93 +/- 97.17\n",
      "Episode length: 277.60 +/- 40.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131250, episode_reward=233.34 +/- 44.88\n",
      "Episode length: 433.90 +/- 260.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=32.31 +/- 117.01\n",
      "Episode length: 850.75 +/- 267.34\n",
      "Eval num_timesteps=138750, episode_reward=-4.14 +/- 60.71\n",
      "Episode length: 968.75 +/- 136.22\n",
      "Eval num_timesteps=142500, episode_reward=14.61 +/- 77.40\n",
      "Episode length: 791.60 +/- 313.48\n",
      "Eval num_timesteps=146250, episode_reward=-37.34 +/- 81.60\n",
      "Episode length: 810.80 +/- 293.59\n",
      "Eval num_timesteps=150000, episode_reward=77.03 +/- 153.45\n",
      "Episode length: 641.95 +/- 361.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "468batch [00:01, 369.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000866 |\n",
      "|    entropy        | 0.866     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 53.8      |\n",
      "|    loss           | 0.688     |\n",
      "|    neglogp        | 0.689     |\n",
      "|    prob_true_act  | 0.553     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "996batch [00:02, 371.03batch/s]\n",
      "Epoch 0 of 2                   \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | -0.00082 |\n",
      "|    entropy        | 0.82     |\n",
      "|    epoch          | 1        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 65       |\n",
      "|    loss           | 0.718    |\n",
      "|    neglogp        | 0.719    |\n",
      "|    prob_true_act  | 0.565    |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1488batch [00:04, 371.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1500     |\n",
      "|    ent_loss       | -0.00086 |\n",
      "|    entropy        | 0.86     |\n",
      "|    epoch          | 1        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 79.1     |\n",
      "|    loss           | 0.843    |\n",
      "|    neglogp        | 0.844    |\n",
      "|    prob_true_act  | 0.507    |\n",
      "|    samples_so_far | 48032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1982batch [00:05, 374.56batch/s]\n",
      "1998batch [00:05, 366.26batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -214.58609955000003\n",
      "Eval num_timesteps=3750, episode_reward=-261.35 +/- 73.35\n",
      "Episode length: 930.05 +/- 210.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-148.70 +/- 37.61\n",
      "Episode length: 407.55 +/- 121.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-109.46 +/- 33.87\n",
      "Episode length: 617.50 +/- 297.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-155.29 +/- 52.06\n",
      "Episode length: 517.35 +/- 307.75\n",
      "Eval num_timesteps=18750, episode_reward=-198.68 +/- 56.67\n",
      "Episode length: 669.05 +/- 252.08\n",
      "Eval num_timesteps=22500, episode_reward=-57.56 +/- 22.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26250, episode_reward=-58.34 +/- 19.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-67.94 +/- 29.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=33750, episode_reward=-66.51 +/- 20.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=37500, episode_reward=-44.14 +/- 17.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=41250, episode_reward=-57.15 +/- 52.87\n",
      "Episode length: 952.50 +/- 113.59\n",
      "Eval num_timesteps=45000, episode_reward=-154.36 +/- 67.61\n",
      "Episode length: 724.15 +/- 236.64\n",
      "Eval num_timesteps=48750, episode_reward=-58.44 +/- 29.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=-48.52 +/- 41.85\n",
      "Episode length: 962.70 +/- 162.59\n",
      "Eval num_timesteps=56250, episode_reward=-78.73 +/- 78.51\n",
      "Episode length: 773.95 +/- 345.54\n",
      "Eval num_timesteps=60000, episode_reward=-103.62 +/- 34.43\n",
      "Episode length: 928.80 +/- 213.62\n",
      "Eval num_timesteps=63750, episode_reward=-71.64 +/- 22.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=-39.17 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71250, episode_reward=-21.70 +/- 21.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-33.87 +/- 19.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=78750, episode_reward=-31.47 +/- 19.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=82500, episode_reward=-24.63 +/- 22.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=86250, episode_reward=-34.94 +/- 22.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-25.68 +/- 22.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=93750, episode_reward=-37.32 +/- 21.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=97500, episode_reward=-42.83 +/- 27.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=101250, episode_reward=-21.10 +/- 25.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=34.68 +/- 83.72\n",
      "Episode length: 926.65 +/- 156.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=108750, episode_reward=-17.31 +/- 24.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=112500, episode_reward=-8.68 +/- 44.01\n",
      "Episode length: 830.85 +/- 295.74\n",
      "Eval num_timesteps=116250, episode_reward=27.25 +/- 168.71\n",
      "Episode length: 314.75 +/- 235.33\n",
      "Eval num_timesteps=120000, episode_reward=110.19 +/- 105.53\n",
      "Episode length: 790.60 +/- 213.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123750, episode_reward=59.14 +/- 108.38\n",
      "Episode length: 863.00 +/- 240.57\n",
      "Eval num_timesteps=127500, episode_reward=118.89 +/- 101.10\n",
      "Episode length: 780.70 +/- 238.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131250, episode_reward=101.66 +/- 120.74\n",
      "Episode length: 782.55 +/- 258.67\n",
      "Eval num_timesteps=135000, episode_reward=128.73 +/- 119.61\n",
      "Episode length: 616.90 +/- 284.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138750, episode_reward=-6.97 +/- 65.78\n",
      "Episode length: 976.70 +/- 96.63\n",
      "Eval num_timesteps=142500, episode_reward=130.14 +/- 110.12\n",
      "Episode length: 669.05 +/- 220.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=146250, episode_reward=119.15 +/- 129.12\n",
      "Episode length: 642.80 +/- 267.23\n",
      "Eval num_timesteps=150000, episode_reward=141.30 +/- 137.68\n",
      "Episode length: 470.15 +/- 308.08\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "480batch [00:01, 362.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000929 |\n",
      "|    entropy        | 0.929     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 54.7      |\n",
      "|    loss           | 0.906     |\n",
      "|    neglogp        | 0.907     |\n",
      "|    prob_true_act  | 0.46      |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "955batch [00:02, 361.80batch/s]\n",
      "992batch [00:02, 357.38batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000779 |\n",
      "|    entropy        | 0.779     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 66.2      |\n",
      "|    loss           | 0.734     |\n",
      "|    neglogp        | 0.735     |\n",
      "|    prob_true_act  | 0.567     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1473batch [00:04, 365.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000809 |\n",
      "|    entropy        | 0.809     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 79.7      |\n",
      "|    loss           | 0.913     |\n",
      "|    neglogp        | 0.914     |\n",
      "|    prob_true_act  | 0.507     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1880batch [00:05, 361.61batch/s]\n",
      "1910batch [00:05, 355.55batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -283.29683975\n",
      "Eval num_timesteps=3750, episode_reward=-90.20 +/- 24.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-18.50 +/- 32.81\n",
      "Episode length: 322.35 +/- 182.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=22.00 +/- 136.11\n",
      "Episode length: 290.80 +/- 78.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-90.12 +/- 25.17\n",
      "Episode length: 456.05 +/- 259.89\n",
      "Eval num_timesteps=18750, episode_reward=-83.79 +/- 31.52\n",
      "Episode length: 827.80 +/- 344.45\n",
      "Eval num_timesteps=22500, episode_reward=-126.97 +/- 19.06\n",
      "Episode length: 377.70 +/- 126.77\n",
      "Eval num_timesteps=26250, episode_reward=-153.53 +/- 35.56\n",
      "Episode length: 485.40 +/- 120.64\n",
      "Eval num_timesteps=30000, episode_reward=-152.07 +/- 47.42\n",
      "Episode length: 969.05 +/- 63.36\n",
      "Eval num_timesteps=33750, episode_reward=-59.94 +/- 23.02\n",
      "Episode length: 870.25 +/- 308.87\n",
      "Eval num_timesteps=37500, episode_reward=-46.88 +/- 24.62\n",
      "Episode length: 876.20 +/- 295.35\n",
      "Eval num_timesteps=41250, episode_reward=-99.92 +/- 36.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-190.05 +/- 132.76\n",
      "Episode length: 853.70 +/- 155.88\n",
      "Eval num_timesteps=48750, episode_reward=-114.51 +/- 107.71\n",
      "Episode length: 987.40 +/- 37.71\n",
      "Eval num_timesteps=52500, episode_reward=-23.86 +/- 17.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=56250, episode_reward=-33.93 +/- 22.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-33.87 +/- 20.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=63750, episode_reward=-77.37 +/- 42.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=-111.79 +/- 22.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=71250, episode_reward=-63.47 +/- 26.48\n",
      "Episode length: 965.05 +/- 152.34\n",
      "Eval num_timesteps=75000, episode_reward=-32.08 +/- 46.74\n",
      "Episode length: 954.30 +/- 139.03\n",
      "Eval num_timesteps=78750, episode_reward=-47.93 +/- 39.23\n",
      "Episode length: 967.35 +/- 100.12\n",
      "Eval num_timesteps=82500, episode_reward=-26.01 +/- 17.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=86250, episode_reward=-8.48 +/- 18.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-60.99 +/- 79.88\n",
      "Episode length: 732.45 +/- 408.71\n",
      "Eval num_timesteps=93750, episode_reward=16.58 +/- 131.64\n",
      "Episode length: 285.40 +/- 257.91\n",
      "Eval num_timesteps=97500, episode_reward=44.37 +/- 113.02\n",
      "Episode length: 731.80 +/- 360.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=101250, episode_reward=-5.88 +/- 126.90\n",
      "Episode length: 793.70 +/- 359.62\n",
      "Eval num_timesteps=105000, episode_reward=35.16 +/- 82.31\n",
      "Episode length: 811.85 +/- 299.16\n",
      "Eval num_timesteps=108750, episode_reward=-15.77 +/- 71.64\n",
      "Episode length: 610.25 +/- 435.26\n",
      "Eval num_timesteps=112500, episode_reward=34.12 +/- 86.43\n",
      "Episode length: 690.90 +/- 391.06\n",
      "Eval num_timesteps=116250, episode_reward=188.47 +/- 93.13\n",
      "Episode length: 391.45 +/- 236.69\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=199.49 +/- 73.01\n",
      "Episode length: 384.40 +/- 144.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=123750, episode_reward=160.13 +/- 106.37\n",
      "Episode length: 591.00 +/- 370.79\n",
      "Eval num_timesteps=127500, episode_reward=210.33 +/- 48.50\n",
      "Episode length: 408.35 +/- 121.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=131250, episode_reward=46.56 +/- 74.32\n",
      "Episode length: 954.45 +/- 98.94\n",
      "Eval num_timesteps=135000, episode_reward=222.30 +/- 48.86\n",
      "Episode length: 543.95 +/- 176.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=138750, episode_reward=180.68 +/- 67.57\n",
      "Episode length: 693.90 +/- 262.17\n",
      "Eval num_timesteps=142500, episode_reward=1.61 +/- 40.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=146250, episode_reward=-15.24 +/- 27.41\n",
      "Episode length: 911.20 +/- 266.55\n",
      "Eval num_timesteps=150000, episode_reward=172.48 +/- 79.44\n",
      "Episode length: 661.70 +/- 251.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00139 |\n",
      "|    entropy        | 1.39     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 40.5     |\n",
      "|    loss           | 1.38     |\n",
      "|    neglogp        | 1.39     |\n",
      "|    prob_true_act  | 0.25     |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "464batch [00:01, 362.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000896 |\n",
      "|    entropy        | 0.896     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 53.5      |\n",
      "|    loss           | 0.746     |\n",
      "|    neglogp        | 0.747     |\n",
      "|    prob_true_act  | 0.539     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "874batch [00:02, 368.91batch/s]\n",
      "985batch [00:02, 365.92batch/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000      |\n",
      "|    ent_loss       | -0.000901 |\n",
      "|    entropy        | 0.901     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 67.3      |\n",
      "|    loss           | 0.797     |\n",
      "|    neglogp        | 0.797     |\n",
      "|    prob_true_act  | 0.525     |\n",
      "|    samples_so_far | 32032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1468batch [00:04, 368.22batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1500      |\n",
      "|    ent_loss       | -0.000806 |\n",
      "|    entropy        | 0.806     |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 82.9      |\n",
      "|    loss           | 0.736     |\n",
      "|    neglogp        | 0.736     |\n",
      "|    prob_true_act  | 0.547     |\n",
      "|    samples_so_far | 48032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1766batch [00:04, 368.71batch/s]\n",
      "1794batch [00:04, 362.83batch/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Timesteps: -280.685424\n",
      "Eval num_timesteps=3750, episode_reward=-201.17 +/- 32.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=-123.88 +/- 78.31\n",
      "Episode length: 386.30 +/- 150.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11250, episode_reward=-193.07 +/- 72.92\n",
      "Episode length: 324.95 +/- 150.75\n",
      "Eval num_timesteps=15000, episode_reward=-187.12 +/- 35.38\n",
      "Episode length: 339.55 +/- 112.22\n",
      "Eval num_timesteps=18750, episode_reward=-156.22 +/- 63.14\n",
      "Episode length: 566.20 +/- 265.76\n",
      "Eval num_timesteps=22500, episode_reward=-53.20 +/- 30.54\n",
      "Episode length: 990.55 +/- 41.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26250, episode_reward=-46.58 +/- 19.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-52.96 +/- 17.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=33750, episode_reward=-137.89 +/- 50.14\n",
      "Episode length: 549.65 +/- 251.62\n",
      "Eval num_timesteps=37500, episode_reward=-100.38 +/- 47.76\n",
      "Episode length: 784.25 +/- 248.00\n",
      "Eval num_timesteps=41250, episode_reward=-85.73 +/- 43.24\n",
      "Episode length: 941.70 +/- 137.82\n",
      "Eval num_timesteps=45000, episode_reward=-224.38 +/- 320.52\n",
      "Episode length: 791.95 +/- 288.45\n",
      "Eval num_timesteps=48750, episode_reward=-79.79 +/- 27.41\n",
      "Episode length: 987.55 +/- 54.27\n",
      "Eval num_timesteps=52500, episode_reward=-42.34 +/- 20.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56250, episode_reward=-19.90 +/- 16.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-61.37 +/- 26.77\n",
      "Episode length: 659.55 +/- 348.16\n",
      "Eval num_timesteps=63750, episode_reward=-70.92 +/- 48.29\n",
      "Episode length: 855.60 +/- 266.41\n",
      "Eval num_timesteps=67500, episode_reward=-50.83 +/- 40.98\n",
      "Episode length: 912.55 +/- 223.13\n"
     ]
    }
   ],
   "source": [
    "reward_sum = 0\n",
    "\n",
    "for seed in seeds:\n",
    "    # create environments and set seed\n",
    "    env_train = gym.make(\"LunarLander-v2\")\n",
    "    env_train.seed(seed)\n",
    "    env_test = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "    env_test.seed(seed)\n",
    "    \n",
    "    # Collect rollouts using the expert\n",
    "    rollouts = rollout.rollout(\n",
    "        heuristic_naive,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(env_train)]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=200),\n",
    "    )\n",
    "    # Flatten the trajectories to obtain individual transitions\n",
    "    transitions = rollout.flatten_trajectories(rollouts)\n",
    "    # Set up student\n",
    "    student = DQN(\n",
    "        policy=dqn.MlpPolicy,\n",
    "        env=env_train,\n",
    "        policy_kwargs = dict(\n",
    "            net_arch = [32,32]\n",
    "        ),\n",
    "        seed=seed,\n",
    "        verbose=False,\n",
    "        **hyperparameter,\n",
    "    )\n",
    "    # Get the layers of the students q_net\n",
    "    student_policy = student.policy\n",
    "    student_layers = [module for module in student.q_net.modules(\n",
    "        ) if isinstance(module, nn.Linear)]\n",
    "    # Set up behavior cloning agent\n",
    "    bc_trainer = bc.BC(\n",
    "        observation_space=env_train.observation_space,\n",
    "        action_space=env_train.action_space,\n",
    "        demonstrations=transitions,\n",
    "    )\n",
    "    # Pretrain the policy with behavior cloning\n",
    "    bc_trainer.train(n_epochs=2)\n",
    "    # Get network parameters of bc expert after training\n",
    "    policy = bc_trainer.policy\n",
    "    expert_shared_layers = [module for module in policy.mlp_extractor.modules(\n",
    "        ) if isinstance(module, nn.Linear)]\n",
    "    expert_output_layers = [module for module in policy.action_net.modules(\n",
    "        ) if isinstance(module, nn.Linear)]\n",
    "    expert_layers = expert_shared_layers + expert_output_layers\n",
    "    # Copy expert parameters to student q_net of student\n",
    "    for (student_layer, expert_layer) in zip(student_layers, expert_layers):\n",
    "        with th.no_grad():\n",
    "            student_layer.weight.copy_(expert_layer.weight)\n",
    "            student_layer.bias.copy_(expert_layer.bias)\n",
    "    student_policy.q_net_target.load_state_dict(student_policy.q_net.state_dict())\n",
    "    # Test performance before training (after pretraining)\n",
    "    reward_before_training, _ = evaluate_policy(student.policy, env_test, 20)\n",
    "    reward_sum += reward_before_training\n",
    "    print(f\"0 Timesteps: {reward_before_training}\")\n",
    "    # Create log folder\n",
    "    trial_log_dir = create_folder(f\"{log_dir}/{seed}\")\n",
    "    # Create eval callback\n",
    "    eval_callback = EvalCallback(\n",
    "        env_test,\n",
    "        log_path=trial_log_dir,\n",
    "        n_eval_episodes=20,\n",
    "        eval_freq=3750,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    # Train the pretrained policy using the regular learning algorithm\n",
    "    student.learn(\n",
    "        total_timesteps=1.5e5,\n",
    "        callback=eval_callback\n",
    "    )\n",
    "np.savez(f\"{log_dir}/zero_scores\", scores=[reward_sum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameter_initialization_deep_rl.common.evaluate import (\n",
    "    create_numpy_arr_from_logs,\n",
    "    create_sample,\n",
    "    mean_confidence_intervals\n",
    ")\n",
    "from parameter_initialization_deep_rl.common.helpers import plot_performance\n",
    "\n",
    "log_dirs = [f\"{log_dir}/{seed}/evaluations.npz\" for seed in seeds]\n",
    "avg_returns, n = create_numpy_arr_from_logs(log_dirs)\n",
    "\n",
    "sample = create_sample(avg_returns)\n",
    "\n",
    "# Save the performance score for the individual seeds\n",
    "np.savez(f\"{log_dir}/avg_perf_across_seeds\",\n",
    "            sample=sample)\n",
    "\n",
    "performance_score = np.average(sample)\n",
    "\n",
    "# Save the performance score, i.e., the averaged return across all seeds and evaluation periods, i.e., just one single number\n",
    "np.savez(f\"{log_dir}/perf_score\",\n",
    "            perf_score=performance_score)\n",
    "\n",
    "AVG, H = mean_confidence_intervals(avg_returns, n)\n",
    "\n",
    "reward_before_training = reward_sum / len(seeds)\n",
    "AVG = np.append(reward_before_training, AVG)\n",
    "\n",
    "# Save the average return and confidence intervals for all the individual evaluation trials averaged across all seeds\n",
    "np.savez(f\"{log_dir}/avg_perf_eval_trials\", avg=AVG,\n",
    "            h=H)\n",
    "\n",
    "plot_performance(\n",
    "    title=\"Performance\",\n",
    "    graph_label=\"Random\",\n",
    "    x = np.arange(0,62*16384,16364),\n",
    "    y=AVG,\n",
    "    x_label=\"timesteps\",\n",
    "    y_label=\"Return\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AVG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dbef0c893a59910565eb9c43a000ae2fe143ab4038181e38f0de48b2404771b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
